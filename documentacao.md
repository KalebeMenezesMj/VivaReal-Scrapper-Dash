# üìò Documenta√ß√£o Definitiva do Projeto de Scraping Imobili√°rio

Este documento serve como a fonte √∫nica de verdade para a arquitetura, opera√ß√£o e desenvolvimento do sistema h√≠brido de coleta e visualiza√ß√£o de dados imobili√°rios.

## 1. Project Overview

| Campo | Detalhe |
| :--- | :--- |
| **Nome do Projeto** | RealEstate Data Harvester (RE-DH) |
| **Resumo de Alto N√≠vel** | Sistema h√≠brido ass√≠ncrono projetado para coletar, processar e visualizar dados de listagens imobili√°rias de fontes externas (e.g., VivaReal) de forma automatizada e escal√°vel. |
| **Objetivos de Neg√≥cio** | Fornecer um *dataset* limpo e atualizado de im√≥veis para an√°lise de mercado, precifica√ß√£o e intelig√™ncia competitiva. Garantir a rastreabilidade completa de cada lote de dados coletado. |
| **P√∫blico Alvo** | Engenheiros de Dados, Analistas de Mercado, Desenvolvedores Full-Stack (para manuten√ß√£o do Frontend/API). |

## 2. Architecture & Design

### 2.1. Padr√£o Arquitet√¥nico
**Padr√£o:** Arquitetura Orientada a Dados (Data-Centric) com Desacoplamento de Processos (Hybrid Asynchronous).

O sistema √© dividido em tr√™s camadas principais que se comunicam atrav√©s de um *Data Hub* centralizado (Supabase), minimizando depend√™ncias diretas entre o motor de I/O intensivo (Python) e a camada de apresenta√ß√£o (React).

### 2.2. Stack Tecnol√≥gica

| Camada | Tecnologia Principal | Prop√≥sito |
| :--- | :--- | :--- |
| **Frontend** | React, TypeScript, Vite | Interface de monitoramento e auditoria. |
| **Data Hub** | Supabase (PostgreSQL) | Persist√™ncia, Autentica√ß√£o, API Gateway (Edge Functions). |
| **Scraping Engine** | Python 3.x, Selenium, BeautifulSoup | Coleta de dados, simula√ß√£o de navegador, parsing. |
| **Infraestrutura** | WebContainer (Desenvolvimento) | Ambiente de execu√ß√£o embutido no navegador. |

### 2.3. Topologia de Implanta√ß√£o (Conceitual)

```mermaid
graph TD
    subgraph Frontend [Camada de Apresenta√ß√£o]
        A[React App] -->|API Calls (Anon Key)| B(Supabase Client)
    end

    subgraph DataHub [Data Hub - Supabase]
        B --> C{PostgreSQL DB}
        D[Edge Function] -->|Service Role Key| C
    end

    subgraph ScraperEngine [Motor de Coleta]
        E[Python Script] -->|Writes/Updates| F(Supabase Client)
    end

    E -- Gera Relat√≥rio --> G[Local Backup (.xlsx)]
    A -- Monitora --> C
```

### 2.4. Decis√µes Chave de Design

*   **Desacoplamento I/O:** O uso do Python/Selenium isola a carga pesada de I/O do ambiente Node.js/Vite, prevenindo bloqueios no servidor de desenvolvimento.
*   **Centraliza√ß√£o de Dados:** O Supabase √© escolhido como *single source of truth*, permitindo que o Frontend consuma dados diretamente via RLS, simplificando a necessidade de um backend REST tradicional.
*   **Robustez do Scraping:** Uso de `webdriver_manager` para garantir que o bin√°rio do Chrome Driver esteja sempre compat√≠vel com a vers√£o do Chrome instalada no ambiente de execu√ß√£o do Python.
*   **Auditoria:** A tabela `scraping_jobs` garante que cada execu√ß√£o de coleta seja rastre√°vel, ligando todos os im√≥veis extra√≠dos a um *Job ID* espec√≠fico.

## 3. System Components

### 3.1. Scraper Engine (`scripts/webscrapping.py`)

*   **Responsabilidade:** Orquestra√ß√£o da coleta, navega√ß√£o, parsing e persist√™ncia em lote.
*   **API/Contrato de Comunica√ß√£o (Supabase):**
    *   **Input:** `max_scrolls` (configura√ß√£o).
    *   **Output (DB Write):** Insere/Atualiza registros em `scraping_jobs` e `properties`.
    *   **Protocolo:** Cliente Python `supabase` (HTTP/REST via Service Key/Anon Key).
*   **Modelos de Dados (Chaves):**
    *   `scraping_jobs`: `{id, status, links_found, properties_scraped, ...}`
    *   `properties`: `{job_id, preco, metragem, rua, bairro, ...}`
*   **L√≥gica Cr√≠tica:** Uso intensivo de `re` (Regex) para normalizar dados n√£o estruturados (pre√ßo, √°rea) e fun√ß√µes de limpeza para padronizar endere√ßos.

### 3.2. Data Hub (Supabase PostgreSQL)

*   **Responsabilidade:** Armazenamento persistente, aplica√ß√£o de regras de neg√≥cio (RLS) e exposi√ß√£o de dados.
*   **Modelos de Dados:** Detalhados na Se√ß√£o 5.1.
*   **Comunica√ß√£o:** Via API REST/Realtime.

### 3.3. Frontend (`src/lib/supabase.ts` e Componentes)

*   **Responsabilidade:** Interface de usu√°rio para visualiza√ß√£o, filtragem e monitoramento do status dos *jobs*.
*   **API/Contrato de Comunica√ß√£o (Supabase):**
    *   **Input:** Credenciais (Anon Key) para inicializa√ß√£o do cliente.
    *   **Output (DB Read):** `supabase.from('scraping_jobs').select()` e `supabase.from('properties').select()`.
    *   **Protocolo:** Cliente JavaScript `@supabase/supabase-js` (HTTP/REST).

## 4. Development & Setup

### 4.1. Pr√©-requisitos de Ferramentas

*   Node.js: Vers√£o 20.x ou superior.
*   Python: Vers√£o 3.10 ou superior.
*   Gerenciador de Pacotes: npm (ou yarn/pnpm).

### 4.2. Configura√ß√£o do Ambiente (Frontend/Node.js)

1.  **Instalar Depend√™ncias:**
    ```bash
    npm install
    ```
2.  **Configurar Vari√°veis de Ambiente:** Crie o arquivo `.env` na raiz do projeto com as chaves do Supabase:
    ```bash
    VITE_SUPABASE_URL=https://levouorsotoewkshrgnk.supabase.co
    VITE_SUPABASE_ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
    ```
3.  **Iniciar o Servidor de Desenvolvimento:**
    ```bash
    npm run dev
    ```
    *(O servidor estar√° acess√≠vel em http://localhost:5173 por padr√£o).*

### 4.3. Configura√ß√£o do Ambiente (Scraper Engine - Python)

**Nota:** Este ambiente deve ser configurado separadamente, idealmente em um ambiente virtual (`venv`).

1.  **Criar Ambiente Virtual (Exemplo):**
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```
2.  **Instalar Depend√™ncias Python:**
    ```bash
    # Instala√ß√£o baseada no conte√∫do do script webscrapping.py
    pip install selenium webdriver-manager pandas openpyxl supabase
    ```
3.  **Configurar Vari√°veis de Ambiente Python:** O script Python tamb√©m requer as chaves do Supabase (pode usar o mesmo `.env` se configurado para ser lido pelo Python, ou um `.env` espec√≠fico para o script).

### 4.4. Execu√ß√£o de Testes

*   **Frontend:** Testes de unidade/integra√ß√£o devem ser implementados usando Jest/Vitest (n√£o configurado, mas recomendado). Executar via `npm test` (se configurado).
*   **Scraper Engine:** Testes unit√°rios para as fun√ß√µes de parsing (`extrair_valores`, `dividir_endereco`) s√£o cruciais.

## 5. Data Management

### 5.1. Schema do Banco de Dados (PostgreSQL)

O schema √© definido pela migra√ß√£o `20251001025023_create_properties_and_jobs_tables.sql`.

**Tabela: `scraping_jobs`**
| Coluna | Tipo | Restri√ß√µes |
| :--- | :--- | :--- |
| `id` | `uuid` | PK, Default `gen_random_uuid()` |
| `status` | `text` | NOT NULL, Default 'pending' |
| `max_scrolls` | `integer` | Default 5 |
| `properties_scraped` | `integer` | Default 0 |
| `created_at`, `completed_at` | `timestamptz` | Timestamps de controle |

**Tabela: `properties`**
| Coluna | Tipo | Restri√ß√µes |
| :--- | :--- | :--- |
| `id` | `uuid` | PK, Default `gen_random_uuid()` |
| `job_id` | `uuid` | FK para `scraping_jobs`, ON DELETE CASCADE |
| `preco`, `metragem`, etc. | `text` | Dados extra√≠dos (normalizados como texto para preservar a formata√ß√£o original do site) |
| `endereco_completo` | `text` | Endere√ßo bruto |

### 5.2. Estrat√©gia de Persist√™ncia

*   **Escrita:** O Python escreve dados em lotes de 10 registros na tabela `properties` para otimizar a lat√™ncia de rede e a carga do banco de dados.
*   **Leitura (Frontend):** O Frontend l√™ diretamente do PostgreSQL via cliente Supabase, aproveitando a infraestrutura gerenciada.

### 5.3. Procedimentos de Migra√ß√£o de Dados

Todas as altera√ß√µes de schema devem seguir o protocolo de migra√ß√£o do Supabase:

1.  Criar um novo arquivo SQL em `/supabase/migrations/` com um timestamp descritivo.
2.  O arquivo deve ser **completo** (DDL total) e incluir `ALTER TABLE ... ENABLE ROW LEVEL SECURITY;`.
3.  Executar a migra√ß√£o usando o CLI do Supabase (fora do escopo do WebContainer, mas procedimento padr√£o).

### 5.4. Backup e Recupera√ß√£o

*   **Backup Prim√°rio:** O script Python gera um backup local (`.xlsx`) de cada execu√ß√£o, servindo como um ponto de recupera√ß√£o imediato e audit√°vel.
*   **Backup Secund√°rio:** O Supabase gerencia backups autom√°ticos do PostgreSQL. A recupera√ß√£o de desastres deve seguir a documenta√ß√£o oficial do Supabase.

## 6. Security Considerations

### 6.1. Autentica√ß√£o e Autoriza√ß√£o

*   **Frontend:** Utiliza a `Anon Key` do Supabase, que restringe o acesso a opera√ß√µes de escrita (INSERT/UPDATE/DELETE) atrav√©s de pol√≠ticas RLS estritas. Apenas `SELECT` √© permitido publicamente.
*   **Scraper Engine (Escrita):** O script Python deve idealmente usar uma chave de servi√ßo (`Service Role Key`) ou uma chave de API gerada por uma Edge Function com permiss√µes elevadas, pois ele precisa inserir dados mesmo quando o RLS est√° ativo para usu√°rios an√¥nimos.

### 6.2. Criptografia

*   **Em Tr√¢nsito:** Todas as comunica√ß√µes (Frontend <-> Supabase, Python <-> Supabase) s√£o for√ßadas a usar **HTTPS/TLS**, garantido pela infraestrutura do Supabase.
*   **Em Repouso:** Os dados no PostgreSQL s√£o criptografados em repouso, conforme padr√£o do servi√ßo Supabase.

### 6.3. Vulnerabilidades e Mitiga√ß√µes

*   **SQL Injection:** Mitigado pelo uso de clientes ORM/SDKs (Supabase JS/Python) que parametrizam consultas, e pela restri√ß√£o de acesso de escrita via RLS/Service Key.
*   **Denial of Service (Scraping):** O `human_sleep` e a limita√ß√£o de *scrolls* (`max_scrolls`) no Python ajudam a mitigar o risco de sobrecarga do servidor de destino e a detec√ß√£o de bots.

## 7. Operational & Maintenance

### 7.1. Logging e Monitoramento

*   **Scraper Engine:** Utiliza `colorama` para logs coloridos no console (INFO, WARNING, ERROR). O status do job (`status='failed'`) no Supabase serve como principal m√©trica de falha operacional.
*   **Frontend:** Logs de erro de rede (falhas de conex√£o com Supabase) devem ser capturados e exibidos ao usu√°rio.
*   **M√©tricas:** A tabela `scraping_jobs` √© o *dashboard* prim√°rio:
    *   **Lat√™ncia:** Diferen√ßa entre `started_at` e `completed_at`.
    *   **Taxa de Sucesso:** `properties_scraped` / `links_found`.

### 7.2. Estrat√©gia de Tratamento de Erros

*   **Erros de Parsing (Python):** Se a extra√ß√£o de um √∫nico im√≥vel falhar, o erro √© logado, o im√≥vel √© pulado, e o *job* continua. O erro √© registrado no campo `error_message` do `scraping_jobs` apenas se for um erro fatal (e.g., falha de conex√£o com o DB).
*   **Erros de Conex√£o (Python):** Tentativas de reconex√£o ou *sleep* exponencial s√£o recomendadas, mas n√£o implementadas no *script* inicial. Falhas cr√≠ticas marcam o job como `status='failed'`.

### 7.3. Procedimentos de Deploy/Rollback

*   **Frontend (Vite):** Deploy via `npm run build` seguido pelo upload dos artefatos est√°ticos para o servi√ßo de hospedagem (e.g., Vercel, Netlify). Rollback √© feito revertendo para a vers√£o anterior dos artefatos est√°ticos.
*   **Scraper Engine (Python):** O script √© executado manualmente ou via um orquestrador externo (e.g., Cron Job, GitHub Actions). Rollback significa simplesmente n√£o executar o script at√© que a vers√£o anterior do c√≥digo seja implantada no ambiente de execu√ß√£o.
